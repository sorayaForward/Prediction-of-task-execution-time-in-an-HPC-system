{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Don't wrap lines\n",
    "pd.set_option('display.expand_frame_repr', False)  # Don't break into multiple lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18306 2937 18306 2937\n"
     ]
    }
   ],
   "source": [
    "load = \"\"\n",
    "X_train = pd.read_csv(f'./datasets_after_clean_2/eagle_data_all_completed_X_train.csv')\n",
    "X_test= pd.read_csv(f'./datasets_after_clean_2/eagle_data_all_completed_X_test.csv')\n",
    "y_train = pd.read_csv(f'./datasets_after_clean_2/eagle_data_all_completed_y_train.csv')\n",
    "y_test = pd.read_csv(f'./datasets_after_clean_2/eagle_data_all_completed_y_test.csv')\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding for qos and partition on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def target_encode_df(df, columns, target_col, base_label=20, step=5):\n",
    "    df= df.copy()\n",
    "    encoding_maps = {}\n",
    "\n",
    "    for col in columns:\n",
    "        # 1. Mean target per category\n",
    "        means = df.groupby(col)[target_col].mean()\n",
    "        \n",
    "        # 2. Sort and assign labels\n",
    "        sorted_cats = means.sort_values().index\n",
    "        label_map = {cat: base_label + i * step for i, cat in enumerate(sorted_cats)}\n",
    "        \n",
    "        # 3. Apply mapping\n",
    "        df[col] = df[col].map(label_map)\n",
    "        encoding_maps[col] = label_map\n",
    "\n",
    "    return df, encoding_maps\n",
    "\n",
    "# ------------------------\n",
    "# üîÅ 1. Merge train & test\n",
    "# ------------------------\n",
    "\n",
    "X_train['__dataset__'] = 'train'\n",
    "X_test['__dataset__'] = 'test'\n",
    "\n",
    "y_train.name = 'run_time'\n",
    "y_test.name = 'run_time'\n",
    "\n",
    "df_train = X_train.copy()\n",
    "df_train['run_time'] = y_train\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test['run_time'] = y_test\n",
    "\n",
    "df_all = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# üß† 2. Target-based label encode\n",
    "# -------------------------------\n",
    "\n",
    "cols_to_encode = ['qos', 'partition']  # your target categorical columns\n",
    "\n",
    "df_all, encodings = target_encode_df(df_all, cols_to_encode, target_col='run_time')\n",
    "\n",
    "# -------------------------------------\n",
    "# üîÅ 3. Split back to X_train and X_test\n",
    "# -------------------------------------\n",
    "\n",
    "X_train= df_all[df_all['__dataset__'] == 'train'].drop(columns=['run_time', '__dataset__'])\n",
    "y_train= df_all[df_all['__dataset__'] == 'train']['run_time']\n",
    "\n",
    "X_test= df_all[df_all['__dataset__'] == 'test'].drop(columns=['run_time', '__dataset__'])\n",
    "y_test= df_all[df_all['__dataset__'] == 'test']['run_time']\n",
    "\n",
    "# ‚úÖ You now have:\n",
    "# X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚úÖ Ensure y_train and y_test are Series with correct name\n",
    "# y_train = y_train.squeeze()\n",
    "# y_test = y_test.squeeze()\n",
    "\n",
    "# y_train.name = 'run_time'\n",
    "# y_test.name = 'run_time'\n",
    "\n",
    "# # ‚úÖ Concatenate cleanly\n",
    "# train_df = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "# test_df = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# df_ = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Job type classification\n",
    "nodes = df_all['nodes_req'].values\n",
    "cpus = df_all['processors_req'].values\n",
    "gpus = df_all['gpus_req'].values\n",
    "\n",
    "conditions = [\n",
    "    (gpus > 0) & (nodes > 1),\n",
    "    (gpus > 0) & (nodes == 1),\n",
    "    (gpus == 0) & (nodes > 1),\n",
    "    (gpus == 0) & (nodes == 1) & (cpus > 1),\n",
    "]\n",
    "choices = [\"distributed_gpu\", \"single_node_gpu\", \"distributed\", \"multithreaded\"]\n",
    "\n",
    "df_all['job_type'] = np.select(conditions, choices, default=\"serial\")\n",
    "job_type_map = {\"serial\": 0, \"multithreaded\": 1, \"distributed\": 2, \"single_node_gpu\": 3, \"distributed_gpu\": 4}\n",
    "\n",
    "df_all['job_type_id'] = df_all['job_type'].map(job_type_map).astype(int)\n",
    "\n",
    "# Generate job_id and composite categorical feature\n",
    "df_all['job_id'] = df_all['job_id'].astype(str)  # if not already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QoS\tSupported Partition(s)\tMax Job Duration\tMax Resources per job\n",
    "debug\tintel, amd, gpu, l40s, condo_amd\t30min\t2 nodes, 2 GPUs\n",
    "normal (default)\tintel, amd, condo_amd\t1 Week\t1024 cores\n",
    "long\tintel, amd, condo_amd\t2 Weeks\t1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['processors_req_nodes'] =df_all['processors_req']/df_all['nodes_req']\n",
    "df_all['processors_req_mem'] =df_all['mem_req']/df_all['processors_req']\n",
    "df_all['nodes_req_mem'] = df_all['mem_req']/df_all['nodes_req']\n",
    "df_all['partition_wallclock'] =df_all['partition'] *df_all['wallclock_req']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get ori sizes\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "\n",
    "# 2. Recreate X and y from df_\n",
    "X_all = df_all.drop(columns=['run_time'])\n",
    "y_all = df_all['run_time']\n",
    "\n",
    "# 3. Resplit exactly as before\n",
    "X_train = X_all.iloc[:n_train].copy()\n",
    "X_test = X_all.iloc[n_train:].copy()\n",
    "\n",
    "y_train = y_all.iloc[:n_train].copy()\n",
    "y_test = y_all.iloc[n_train:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in ['user_account_name']]\n",
    "\n",
    "def factorize_train_then_test_dynamic(dataset, cat_columns):\n",
    "    mapping_dict = {}\n",
    "\n",
    "    for col in cat_columns:\n",
    "        uniques_train = dataset[col].astype(str).unique()\n",
    "        cat_to_int = {k: i for i, k in enumerate(uniques_train)}\n",
    "\n",
    "        dataset[f\"{col}\"] = dataset[col].astype(str).map(cat_to_int)\n",
    "        mapping_dict[col] = cat_to_int\n",
    "\n",
    "    return dataset, mapping_dict\n",
    "\n",
    "\n",
    "def apply_factorization_to_test(dataset, mapping_dict):\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    for col, cat_to_int in mapping_dict.items():\n",
    "        # make sure ‚ÄúOther‚Äù is in the mapping\n",
    "        if 'Other' not in cat_to_int:\n",
    "            raise KeyError(f\"'Other' not found in mapping for column {col}\")\n",
    "\n",
    "        other_code = cat_to_int['Other']\n",
    "\n",
    "        # map unseen values to other_code\n",
    "        dataset[col] = (dataset[col]\n",
    "                        .astype(str)\n",
    "                        .map(lambda x: cat_to_int.get(x, other_code))\n",
    "                        .astype(int)\n",
    "                       )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "X_train, mappings = factorize_train_then_test_dynamic(\n",
    "    X_train, categorical_features\n",
    ")\n",
    "X_test = apply_factorization_to_test(X_test, mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'datasets_after_transformation' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory name\n",
    "directory_name = \"datasets_after_transformation\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory_name):\n",
    "    os.makedirs(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets_after_transformation/eagle_data_all_completed_X_train.csv', index=False)\n",
    "X_test.to_csv('datasets_after_transformation/eagle_data_all_completed_X_test.csv', index=False)\n",
    "y_train.to_csv('datasets_after_transformation/eagle_data_all_completed_y_train.csv', index=False)\n",
    "y_test.to_csv('datasets_after_transformation/eagle_data_all_completed_y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
