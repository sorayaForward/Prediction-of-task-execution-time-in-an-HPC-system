{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Here were cleaning goes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Don't wrap lines\n",
    "pd.set_option('display.expand_frame_repr', False)  # Don't break into multiple lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"datasets_after_clean_1/eagle_data_all_completed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>wallclock_req</th>\n",
       "      <th>nodes_req</th>\n",
       "      <th>processors_req</th>\n",
       "      <th>gpus_req</th>\n",
       "      <th>mem_req</th>\n",
       "      <th>run_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.936330e+05</td>\n",
       "      <td>293633.000000</td>\n",
       "      <td>293633.000000</td>\n",
       "      <td>293633.000000</td>\n",
       "      <td>293633.000000</td>\n",
       "      <td>2.936330e+05</td>\n",
       "      <td>293633.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.146731e+07</td>\n",
       "      <td>45296.287611</td>\n",
       "      <td>1.330634</td>\n",
       "      <td>18.901598</td>\n",
       "      <td>0.032670</td>\n",
       "      <td>1.346334e+05</td>\n",
       "      <td>5024.657777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.053431e+05</td>\n",
       "      <td>72034.289642</td>\n",
       "      <td>2.565700</td>\n",
       "      <td>72.158847</td>\n",
       "      <td>0.406572</td>\n",
       "      <td>2.840960e+05</td>\n",
       "      <td>21649.960450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.129360e+07</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.137537e+07</td>\n",
       "      <td>14400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.524800e+04</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.146445e+07</td>\n",
       "      <td>14400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.524800e+04</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.154664e+07</td>\n",
       "      <td>36000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.524800e+04</td>\n",
       "      <td>905.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.168475e+07</td>\n",
       "      <td>864000.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>5400.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.704960e+07</td>\n",
       "      <td>827806.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             job_id  wallclock_req      nodes_req  processors_req       gpus_req       mem_req       run_time\n",
       "count  2.936330e+05  293633.000000  293633.000000   293633.000000  293633.000000  2.936330e+05  293633.000000\n",
       "mean   1.146731e+07   45296.287611       1.330634       18.901598       0.032670  1.346334e+05    5024.657777\n",
       "std    1.053431e+05   72034.289642       2.565700       72.158847       0.406572  2.840960e+05   21649.960450\n",
       "min    1.129360e+07      60.000000       1.000000        1.000000       0.000000  5.000000e+03       3.000000\n",
       "25%    1.137537e+07   14400.000000       1.000000        1.000000       0.000000  8.524800e+04      50.000000\n",
       "50%    1.146445e+07   14400.000000       1.000000        1.000000       0.000000  8.524800e+04      82.000000\n",
       "75%    1.154664e+07   36000.000000       1.000000       24.000000       0.000000  8.524800e+04     905.000000\n",
       "max    1.168475e+07  864000.000000     200.000000     5400.000000      32.000000  1.704960e+07  827806.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_id', 'user', 'account', 'partition', 'qos', 'wallclock_req',\n",
       "       'nodes_req', 'processors_req', 'gpus_req', 'mem_req', 'submit_time',\n",
       "       'end_time', 'run_time', 'name', 'work_dir', 'submit_line',\n",
       "       'job_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct job_id ( clusterize jobs having same into one cluster to make model deffrentiate between normal jobs and job arrays, generally having close runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_cols = [\n",
    "    'wallclock_req', 'nodes_req', 'processors_req', 'partition', 'qos',\n",
    "    'gpus_req', 'mem_req', 'submit_line', 'work_dir', 'name','user', 'account',\n",
    "]\n",
    "\n",
    "# Step 1: Generate a group ID based on resource configuration\n",
    "df_['group_id'] = df_.groupby(resource_cols).ngroup()\n",
    "\n",
    "# Step 2: Count how many times each group_id appears\n",
    "group_counts = df_['group_id'].value_counts()\n",
    "\n",
    "# Step 3: Keep original job_id if group is unique; otherwise use group_id\n",
    "df_['job_id'] = df_.apply(\n",
    "    lambda row: row['group_id'] if group_counts[row['group_id']] > 1 else row['job_id'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 4: (Optional) drop group_id column\n",
    "df_.drop(columns='group_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (290696, 16)\n",
      "X_test shape: (2937, 16)\n",
      "y_train shape: (290696,)\n",
      "y_test shape: (2937,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sort data by submit_time ascending\n",
    "df_sorted = df_.sort_values('submit_time').reset_index(drop=True)\n",
    "\n",
    "# Calculate split index\n",
    "split_index = int(len(df_) * 0.99)\n",
    "\n",
    "# Define target column name\n",
    "target_col = 'run_time'\n",
    "\n",
    "# Split features and target\n",
    "X = df_sorted.drop(columns=[target_col])\n",
    "y = df_sorted[target_col]\n",
    "\n",
    "# Split into train and test sets chronologically\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qos\n",
       "normal     267175\n",
       "high        17135\n",
       "standby      6386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['qos'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete outliers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate IQR bounds\n",
    "# Q1 = y_train.quantile(0.25)\n",
    "# Q3 = y_train.quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# lower_bound = Q1 - 1.5 * IQR\n",
    "# upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# # Identify outliers\n",
    "# outliers_mask = (y_train < lower_bound) | (y_train > upper_bound)\n",
    "\n",
    "# # Print number of outliers\n",
    "# print(\"Nbr of outliers:\", outliers_mask.sum())\n",
    "\n",
    "# # Access outlier runtimes\n",
    "# outlier_y = y_train[outliers_mask]\n",
    "\n",
    "# # Min/Max runtime among outliers (in hours)\n",
    "# print(\"Min outlier run_time (hrs):\", outlier_y.min() / 3600)\n",
    "# print(\"Max outlier run_time (hrs):\", outlier_y.max() / 3600)\n",
    "\n",
    "# # Remove outliers for training\n",
    "# X_train = X_train[~outliers_mask]\n",
    "# y_train = y_train[~outliers_mask]\n",
    "\n",
    "# # Min/Max runtime after removing outliers (in hours)\n",
    "# print(\"Min run_time (hrs):\", y_train.min() / 3600)\n",
    "# print(\"Max run_time (hrs):\", y_train.max() / 3600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qos\n",
      "high       6386\n",
      "normal     6386\n",
      "standby    6386\n",
      "Name: count, dtype: int64\n",
      "X_train shape: (19158, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Split by qos category\n",
    "X_train_normal = X_train[X_train['qos'] == 'normal']\n",
    "X_train_high = X_train[X_train['qos'] == 'high']\n",
    "X_train_standby = X_train[X_train['qos'] == 'standby']\n",
    "\n",
    "y_train_normal = y_train.loc[X_train_normal.index]\n",
    "y_train_high = y_train.loc[X_train_high.index]\n",
    "y_train_standby = y_train.loc[X_train_standby.index]\n",
    "\n",
    "# ✅ Step 2: Downsample 'high' to match smallest class (or define target_size manually)\n",
    "target_size = min(len(X_train_normal), len(X_train_standby))  # Or set: target_size = 1000\n",
    "\n",
    "X_train_high_downsampled = X_train_high.sample(n=target_size, random_state=42)\n",
    "y_train_high_downsampled = y_train_high.loc[X_train_high_downsampled.index]\n",
    "\n",
    "# ✅ Step 3: Recombine: keep normal + standby, only use downsampled high\n",
    "X_train = pd.concat([X_train_normal, X_train_standby, X_train_high_downsampled], ignore_index=True)\n",
    "y_train = pd.concat([y_train_normal, y_train_standby, y_train_high_downsampled], ignore_index=True)\n",
    "\n",
    "# Step 2: Downsample 'normal' to same size as minority class\n",
    "target_size = min(len(X_train_standby), len(X_train_high)) \n",
    "\n",
    "X_train_normal_downsampled = X_train_normal.sample(n=target_size, random_state=42)\n",
    "y_train_normal_downsampled = y_train_normal.loc[X_train_normal_downsampled.index]\n",
    "\n",
    "# Recombine all downsampled sets\n",
    "X_train = pd.concat([X_train_normal_downsampled, X_train_standby, X_train_high_downsampled], ignore_index=True)\n",
    "y_train = pd.concat([y_train_normal_downsampled, y_train_standby, y_train_high_downsampled], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Step 4: Shuffle both X and y together\n",
    "shuffled = X_train.copy()\n",
    "shuffled['__y__'] = y_train\n",
    "shuffled = shuffled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ✅ Step 5: Final X_train and y_train\n",
    "y_train = shuffled.pop('__y__')\n",
    "X_train = shuffled\n",
    "\n",
    "# ✅ Final check\n",
    "print(X_train['qos'].value_counts())\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition\n",
      "partition007    6051\n",
      "partition001    5480\n",
      "partition026    3329\n",
      "partition028    3002\n",
      "partition006     444\n",
      "Name: count, dtype: int64\n",
      "X_train shape: (18306, 16)\n",
      "y_train shape: (18306,)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get top 5 partitions by count\n",
    "top_partitions = X_train['partition'].value_counts().nlargest(5).index\n",
    "\n",
    "# Step 2: Create mask and apply to both X_train and y_train\n",
    "mask = X_train['partition'].isin(top_partitions)\n",
    "\n",
    "X_train = X_train[mask].copy()\n",
    "y_train = y_train[mask].copy()\n",
    "\n",
    "# Optional: Check result\n",
    "print(X_train['partition'].value_counts())\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soraya Khene\\AppData\\Local\\Temp\\ipykernel_21340\\1016810155.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['user_account_name'] = X_test['user'] + '@' + X_test['account'] + '@' + X_test['name']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_account_name\n",
       "Other                                9605\n",
       "user0716@account0464@name00295747    3183\n",
       "user0095@account0529@name00011523    2531\n",
       "user0295@account0472@name02650679    1072\n",
       "user0776@account0402@name00015238     858\n",
       "user0864@account0072@name02494232     366\n",
       "user0819@account0033@name00004484     258\n",
       "user0227@account0033@name00004484     221\n",
       "user0322@account0180@name00059927     212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['user_account_name'] = X_train['user'] + '@' + X_train['account'] + '@' + X_train['name']\n",
    "user_counts = X_train['user_account_name'].value_counts()\n",
    "\n",
    "X_test['user_account_name'] = X_test['user'] + '@' + X_test['account'] + '@' + X_test['name']\n",
    "threshold = 200  # Keep users with more than 200 jobs\n",
    "frequent_users = user_counts[user_counts > threshold].index\n",
    "\n",
    "X_train['user_account_name'] = X_train['user_account_name'].apply(lambda x: x if x in frequent_users else 'Other')\n",
    "X_train['user_account_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'datasets_after_clean_2' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory name\n",
    "directory_name = \"datasets_after_clean_2\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory_name):\n",
    "    os.makedirs(directory_name)\n",
    "    print(f\"Directory '{directory_name}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('datasets_after_clean_2/eagle_data_all_completed_X_train.csv', index=False)\n",
    "X_test.to_csv('datasets_after_clean_2/eagle_data_all_completed_X_test.csv', index=False)\n",
    "y_train.to_csv('datasets_after_clean_2/eagle_data_all_completed_y_train.csv', index=False)\n",
    "y_test.to_csv('datasets_after_clean_2/eagle_data_all_completed_y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
